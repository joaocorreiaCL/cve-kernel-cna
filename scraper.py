#!/usr/bin/python

import os
import requests
from bs4 import BeautifulSoup

# Base URL of the website
base_url = "https://lore.kernel.org/linux-cve-announce/"

# Directory to store CVE files
cve_directory = "CVEs"
os.makedirs(cve_directory, exist_ok=True)

def get_page_content(url):
    response = requests.get(url)
    response.raise_for_status()  # Raises an HTTPError for bad responses
    return response.text

def save_cve_content(cve_id, content):
    filename = os.path.join(cve_directory, f"{cve_id}.txt")
    if not os.path.exists(filename):  # Skip already existing files
        with open(filename, "w") as file:
            file.write(content)
    return filename

def scrape_page(url):
    while url:
        print(f"Scraping: {url}")
        html_content = get_page_content(url)
        soup = BeautifulSoup(html_content, 'html.parser')

        # Extract all CVE entries
        for link in soup.find_all('a'):
            if 'CVE' in link.text and 'REJECTED' not in link.text:
                href = link.get('href', '')
                cve_id = link.text.split(':')[0].strip()  # Extracts CVE ID correctly from the link text
                if 'CVE' in cve_id:
                    full_link = base_url + href
                    cve_content = get_page_content(full_link)
                    filename = save_cve_content(cve_id, cve_content)
                    print(f"Downloaded {cve_id}: saved to {filename}")

        # Find the next page link
        next_link = soup.find('a', rel='next')
        url = base_url + next_link['href'] if next_link else None

# Start scraping from the main page
scrape_page(base_url)

print("Scraping completed.")
