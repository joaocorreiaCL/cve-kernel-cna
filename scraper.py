#!/usr/bin/python

import os
import requests
import time
from bs4 import BeautifulSoup

# Base URLs
base_url = "https://lore.kernel.org/linux-cve-announce/"
cve_api_url = "https://cveawg.mitre.org/api/cve/"

# Directories to store CVE files and JSON files
cve_directory = "CVEs"
json_directory = "json"
os.makedirs(cve_directory, exist_ok=True)
os.makedirs(json_directory, exist_ok=True)

def get_page_content(url):
    response = requests.get(url)
    response.raise_for_status()  # Raises an HTTPError for bad responses
    return response.text

def save_cve_content(cve_id, content):
    filename = os.path.join(cve_directory, f"{cve_id}.txt")
    if not os.path.exists(filename):  # Skip already existing files
        with open(filename, "w") as file:
            file.write(content)
    return filename

def save_cve_json(cve_id, json_content):
    filename = os.path.join(json_directory, f"{cve_id}.json")
    if not os.path.exists(filename):  # Skip already existing files
        with open(filename, "w") as file:
            file.write(json_content)
    return filename

def scrape_page(url):
    while url:
        print(f"Scraping: {url}")
        html_content = get_page_content(url)
        soup = BeautifulSoup(html_content, 'html.parser')

        # Extract all CVE entries
        for link in soup.find_all('a'):
            if 'CVE' in link.text and 'REJECTED' not in link.text:
                href = link.get('href', '')
                cve_id = link.text.split(':')[0].strip()  # Extracts CVE ID correctly from the link text
                if 'CVE' in cve_id:
                    full_link = base_url + href

                    # Save CVE content if not already present
                    filename = os.path.join(cve_directory, f"{cve_id}.txt")
                    if not os.path.exists(filename):
                        cve_content = get_page_content(full_link)
                        save_cve_content(cve_id, cve_content)
                        print(f"Downloaded {cve_id}: saved to {filename}")
                    else:
                        print(f"{cve_id} text file already exists: {filename}")

                    # Fetch and save JSON data for the CVE if not already present
                    json_filename = os.path.join(json_directory, f"{cve_id}.json")
                    if not os.path.exists(json_filename):
                        cve_json_url = cve_api_url + cve_id
                        try:
                            cve_json_content = get_page_content(cve_json_url)
                            save_cve_json(cve_id, cve_json_content)
                            print(f"Downloaded {cve_id} JSON: saved to {json_filename}")
                            time.sleep(2)  # 2-second delay between API requests
                        except requests.RequestException as e:
                            print(f"Failed to download JSON for {cve_id}: {e}")
                    else:
                        print(f"{cve_id} JSON file already exists: {json_filename}")

        # Find the next page link
        next_link = soup.find('a', rel='next')
        url = base_url + next_link['href'] if next_link else None

# Start scraping from the main page
scrape_page(base_url)

print("Scraping completed.")
